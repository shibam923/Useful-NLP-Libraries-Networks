{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: Compare and contrast NLTK and spaCy in terms of features, ease of use,\n",
        "and performance.\n",
        "\n",
        "Answer:\n",
        "NLTK and spaCy are popular Python libraries for natural language processing (NLP), but they target different needs: NLTK emphasizes educational flexibility, while spaCy prioritizes production efficiency.\n",
        "\n",
        "## Key Features\n",
        "NLTK offers a broad suite including tokenization, stemming, lemmatization, POS tagging, NER, sentiment analysis, emotion detection, language detection, and access to large corpora.\n",
        "\n",
        "spaCy provides non-destructive tokenization, POS tagging, dependency parsing, NER, text classification, sentence segmentation, and pre-trained models for 19+ languages with multi-task learning support.\n",
        "\n",
        "Both support core tasks like tokenization and NER, but NLTK excels in corpora and customization, while spaCy focuses on trainable pipelines and visualizers.\n",
        "\n",
        "## Ease of Use\n",
        "NLTK has a steeper learning curve due to its modular, code-heavy approach, suiting learners who want to explore algorithms deeply.\n",
        "\n",
        "spaCy features a simple, consistent API for quick implementation, with less code needed for common tasks, though custom training can be complex.\n",
        "\n",
        "NLTK requires more setup for production but offers extensive documentation; spaCy is more intuitive for beginners in applied NLP.\n",
        "\n",
        "## Performance\n",
        "spaCy is significantly faster, processing 10,000+ tokens per second via Cython optimization, ideal for large-scale or real-time use—up to 50x quicker than NLTK on tokenization.\n",
        "\n",
        "NLTK is slower (around 2,000 tokens/second) due to pure Python, better for prototyping or small datasets.\n",
        "\n",
        "spaCy often delivers higher accuracy (90-95% on benchmarks) with modern models; NLTK matches for specific tasks but needs tuning.\n",
        "\n",
        "## Comparison Table\n",
        "\n",
        "| Aspect          | NLTK                                      | spaCy                                      |\n",
        "|-----------------|-------------------------------------------|--------------------------------------------|\n",
        "| **Strengths**   | Educational, flexible, vast corpora | Production-ready, fast, accurate models  |\n",
        "| **Weaknesses**  | Slower, steeper curve               | Less flexible for research, memory-heavy  |\n",
        "| **Best For**    | Learning/research                 | Apps/large data                      |"
      ],
      "metadata": {
        "id": "JrxCm54aSWI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is TextBlob and how does it simplify common NLP tasks like\n",
        "sentiment analysis and translation?\n",
        "\n",
        "Answer: TextBlob is a simple Python library built on NLTK and Pattern, designed for straightforward text processing in NLP. It provides an intuitive, \"Pythonic\" interface that abstracts complex operations into one-liners.\n",
        "\n",
        "## Core Features\n",
        "TextBlob supports tokenization (words/sentences), POS tagging, noun phrase extraction, sentiment analysis, classification (Naive Bayes/Decision Tree), n-grams, lemmatization, spelling correction, and WordNet integration.\n",
        "\n",
        "It also includes translation and language detection powered by Google Translate, plus word inflection for plurals/singulars.\n",
        "\n",
        "## Simplifying Sentiment Analysis\n",
        "Sentiment analysis returns polarity (-1.0 negative to 1.0 positive) and subjectivity (0.0 objective to 1.0 subjective) with a single call: `TextBlob(text).sentiment`. This uses a rule-based approach from Pattern, avoiding manual model training for quick prototyping.\n",
        "\n",
        "Example: `TextBlob(\"Python is great!\").sentiment` yields (0.8, 0.9), indicating positive, subjective text.\n",
        "\n",
        "## Simplifying Translation\n",
        "Translation is a one-liner: `TextBlob(text).translate(to='es')` detects source language and converts using Google Translate API—no setup required.\n",
        "\n",
        "Example: `TextBlob(\"Hello world\").translate(to='fr')` outputs \"Bonjour le monde.\"\n",
        "\n",
        "## Key Advantages\n",
        "Its simplicity suits beginners and rapid tasks, requiring minimal code versus raw NLTK, though it's slower without neural models."
      ],
      "metadata": {
        "id": "03tyNYKSTsMz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: Explain the role of Standford NLP in academic and industry NLP Projects.\n",
        "\n",
        "Answer:  Stanford NLP, primarily through tools like Stanford CoreNLP and Stanza, provides robust, research-grade software for linguistic annotations in text processing. It plays a foundational role in both advancing theoretical NLP and enabling practical applications.\n",
        "\n",
        "## Academic Role\n",
        "In academia, Stanford NLP drives basic research in computational linguistics, machine learning, and cognitive science, supporting over 60 languages via Stanza.\n",
        "\n",
        "It powers educational materials, student projects, and peer-reviewed papers, with CoreNLP used for tasks like dependency parsing and coreference resolution in experiments.\n",
        "\n",
        "The Stanford NLP Group fosters interdisciplinary work, influencing curricula and tools for training future researchers.\n",
        "\n",
        "## Industry Role\n",
        "Industry adopts Stanford tools for production systems in sentiment analysis, NER, relation extraction, and text classification, integrating them into apps for customer feedback and search.\n",
        "\n",
        "Widely used by companies for scalable NLP pipelines, it bridges research to deployment despite resource constraints in academia.\n",
        "\n",
        "## Key Impacts\n",
        "Stanford NLP sets benchmarks for accuracy in parsing and annotation, enabling hybrid academia-industry collaborations that boost innovation.\n",
        "\n",
        "| Context     | Primary Contributions                  | Examples                          |\n",
        "|-------------|----------------------------------------|-----------------------------------|\n",
        "| **Academic**| Research tools, education, experiments | Stanza for 60+ languages, papers  |\n",
        "| **Industry**| Applied annotations, scalable apps  | Sentiment, NER in products  |"
      ],
      "metadata": {
        "id": "U1xeNlEMUOEd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Describe the architecture and functioning of a Recurrent Natural Network\n",
        "(RNN).\n",
        "\n",
        "Answer: Recurrent Neural Networks (RNNs) are neural networks designed for sequential data, where connections form cycles to maintain a \"memory\" of previous inputs. They process inputs step-by-step, updating an internal state to capture dependencies over time.\n",
        "\n",
        "## Core Architecture\n",
        "RNNs consist of three main layers: an input layer for sequential data (like words in a sentence), a hidden (recurrent) layer that holds the state, and an output layer for predictions.\n",
        "\n",
        "The key is the recurrent connection: at each time step \\( t \\), the hidden state \\( h_t \\) is computed as \\( h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h) \\), where \\( x_t \\) is the current input, \\( h_{t-1} \\) is the prior hidden state, \\( W \\) are weight matrices, and \\( b_h \\) is bias.\n",
        "\n",
        "This creates a chain-like unfolding over time, sharing weights across steps for efficiency.\n",
        "\n",
        "## Functioning Process\n",
        "RNNs process sequences iteratively: start with initial hidden state \\( h_0 \\) (often zeros), feed each input \\( x_t \\) to update \\( h_t \\), then compute output \\( y_t = W_{hy} h_t + b_y \\).\n",
        "\n",
        "During training, backpropagation through time (BPTT) unrolls the network, computing gradients across steps to optimize parameters.\n",
        "\n",
        "They excel at tasks like language modeling but suffer from vanishing gradients on long sequences.\n",
        "\n",
        "## Comparison of Components\n",
        "\n",
        "| Component      | Role                                      | Example Equation/Note                  |\n",
        "|----------------|-------------------------------------------|----------------------------------------|\n",
        "| **Input**      | Current sequence element           | \\( x_t \\) (e.g., word embedding)      |\n",
        "| **Hidden State** | Memory from prior steps      | \\( h_t = f(h_{t-1}, x_t) \\)           |\n",
        "| **Output**     | Prediction per step               | \\( y_t \\) for classification/seq gen  |\n",
        "| **Weights**    | Learned parameters (shared)    | \\( W_{xh}, W_{hh}, W_{hy} \\)          |"
      ],
      "metadata": {
        "id": "JaHePbf4UyLi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is the key difference between LSTM and GRU networks in NLP\n",
        "applications?\n",
        "\n",
        "Answer: LSTM and GRU are both gated RNN variants that mitigate vanishing gradients, but their key difference lies in gating mechanisms and architectural simplicity, impacting efficiency in NLP tasks like sequence modeling.\n",
        "\n",
        "## Gating Mechanisms\n",
        "LSTM employs three gates—input, forget, and output—plus a distinct cell state to regulate information flow, enabling precise control over long-term dependencies.\n",
        "\n",
        "GRU streamlines this with two gates—reset and update—merging cell and hidden states into one, reducing parameters by about 25% for faster computation.\n",
        "\n",
        "## NLP Applications\n",
        "In NLP, LSTMs shine in tasks needing deep context retention, such as document-level sentiment analysis or machine translation with long sequences.\n",
        "\n",
        "GRUs excel in resource-constrained or real-time scenarios like NER or chatbots, offering comparable accuracy with quicker training and inference.\n",
        "\n",
        "## Comparison Table\n",
        "\n",
        "| Aspect          | LSTM                               | GRU                                |\n",
        "|-----------------|------------------------------------|------------------------------------|\n",
        "| **Gates**       | 3 (input, forget, output)  | 2 (reset, update)          |\n",
        "| **States**      | Hidden + cell          | Hidden only               |\n",
        "| **Parameters**  | More (slower)              | Fewer (faster)             |\n",
        "| **NLP Best For**| Long sequences         | Efficiency/real-time       |"
      ],
      "metadata": {
        "id": "jPVI_S4IVUqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program using TextBlob to perform sentiment analysis on\n",
        "the following paragraph of text:\n",
        "\n",
        "“I had a great experience using the new mobile banking app. The interface is intuitive,\n",
        "and customer support was quick to resolve my issue. However, the app did crash once\n",
        "during a transaction, which was frustrating\"\n",
        "\n",
        "Your program should print out the polarity and subjectivity scores.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "xADbpoLUV8rd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NoxWuDMSR45",
        "outputId": "7e8300bd-e9c5-4635-81bc-1757a53188ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.21742424242424244\n",
            "Subjectivity: 0.6511363636363636\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Given text\n",
        "text = (\n",
        "    \"I had a great experience using the new mobile banking app. \"\n",
        "    \"The interface is intuitive, and customer support was quick to resolve my issue. \"\n",
        "    \"However, the app did crash once during a transaction, which was frustrating.\"\n",
        ")\n",
        "\n",
        "# Create TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Get sentiment scores\n",
        "polarity = blob.sentiment.polarity\n",
        "subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "# Print results\n",
        "print(\"Polarity:\", polarity)\n",
        "print(\"Subjectivity:\", subjectivity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics,\n",
        "computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "B8mnOwIfWg9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download tokenizer (only needed first time)\n",
        "nltk.download('punkt_tab') # Changed from 'punkt' to 'punkt_tab' as indicated by the error\n",
        "\n",
        "# Given paragraph\n",
        "text = (\n",
        "    \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, \"\n",
        "    \"computer science, and artificial intelligence. It enables machines to understand, \"\n",
        "    \"interpret, and generate human language. Applications of NLP include chatbots, \"\n",
        "    \"sentiment analysis, and machine translation. As technology advances, the role of NLP \"\n",
        "    \"in modern solutions is becoming increasingly critical.\"\n",
        ")\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Frequency distribution\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Print tokens\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# Print frequency distribution\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "for word, freq in freq_dist.items():\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8Ig4QYGWsYW",
        "outputId": "9b9d9562-a6f7-4591-b60b-a591313fd04f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Frequency Distribution:\n",
            "Natural: 1\n",
            "Language: 1\n",
            "Processing: 1\n",
            "(: 1\n",
            "NLP: 3\n",
            "): 1\n",
            "is: 2\n",
            "a: 1\n",
            "fascinating: 1\n",
            "field: 1\n",
            "that: 1\n",
            "combines: 1\n",
            "linguistics: 1\n",
            ",: 7\n",
            "computer: 1\n",
            "science: 1\n",
            "and: 3\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            ".: 4\n",
            "It: 1\n",
            "enables: 1\n",
            "machines: 1\n",
            "to: 1\n",
            "understand: 1\n",
            "interpret: 1\n",
            "generate: 1\n",
            "human: 1\n",
            "language: 1\n",
            "Applications: 1\n",
            "of: 2\n",
            "include: 1\n",
            "chatbots: 1\n",
            "sentiment: 1\n",
            "analysis: 1\n",
            "machine: 1\n",
            "translation: 1\n",
            "As: 1\n",
            "technology: 1\n",
            "advances: 1\n",
            "the: 1\n",
            "role: 1\n",
            "in: 1\n",
            "modern: 1\n",
            "solutions: 1\n",
            "becoming: 1\n",
            "increasingly: 1\n",
            "critical: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Implement a basic LSTM model in Keras for a text classification task using\n",
        "the following dummy dataset. Your model should classify sentences as either positive\n",
        "(1) or negative (0).\n",
        "\n",
        "# Dataset\n",
        "texts = [\n",
        "“I love this project”, #Positive\n",
        "“This is an amazing experience”, #Positive\n",
        "“I hate waiting in line”, #Negative\n",
        "“This is the worst service”, #Negative\n",
        "“Absolutely fantastic!” #Positive\n",
        "]\n",
        "\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "\n",
        "Preprocess the text, tokenize it, pad sequences, and build an LSTM model to train on\n",
        "this data. You may use Keras with TensorFlow backend.  \n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:  "
      ],
      "metadata": {
        "id": "_9DI93zkXGzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Dataset\n",
        "texts = [\n",
        "    \"I love this project\",                 # Positive\n",
        "    \"This is an amazing experience\",       # Positive\n",
        "    \"I hate waiting in line\",               # Negative\n",
        "    \"This is the worst service\",            # Negative\n",
        "    \"Absolutely fantastic!\"                # Positive\n",
        "]\n",
        "\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# Padding sequences\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=10, input_length=max_len),\n",
        "    LSTM(16),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    padded_sequences,\n",
        "    labels,\n",
        "    epochs=20,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Test prediction\n",
        "test_text = [\"I really love this experience\"]\n",
        "test_seq = tokenizer.texts_to_sequences(test_text)\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "prediction = model.predict(test_pad)\n",
        "\n",
        "print(\"\\nPrediction for test sentence:\", prediction)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2-mLJB-9XoG1",
        "outputId": "4f7322da-1a8c-4c4a-f4a2-8ee8012dbba0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.6000 - loss: 0.6920\n",
            "Epoch 2/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6000 - loss: 0.6906\n",
            "Epoch 3/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6892\n",
            "Epoch 4/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6877\n",
            "Epoch 5/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6863\n",
            "Epoch 6/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6848\n",
            "Epoch 7/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6000 - loss: 0.6833\n",
            "Epoch 8/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - accuracy: 0.6000 - loss: 0.6817\n",
            "Epoch 9/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6801\n",
            "Epoch 10/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6784\n",
            "Epoch 11/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6767\n",
            "Epoch 12/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6000 - loss: 0.6749\n",
            "Epoch 13/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.6000 - loss: 0.6730\n",
            "Epoch 14/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - accuracy: 0.6000 - loss: 0.6710\n",
            "Epoch 15/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6690\n",
            "Epoch 16/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6668\n",
            "Epoch 17/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - accuracy: 0.6000 - loss: 0.6645\n",
            "Epoch 18/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.6000 - loss: 0.6622\n",
            "Epoch 19/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.6000 - loss: 0.6597\n",
            "Epoch 20/20\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 0.6000 - loss: 0.6571\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 185ms/step\n",
            "\n",
            "Prediction for test sentence: [[0.5467706]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Using spaCy, build a simple NLP pipeline that includes tokenization,\n",
        "lemmatization, and entity recognition. Use the following paragraph as your dataset:\n",
        "\n",
        "“Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the\n",
        "development of India’s atomic energy program. He was the founding director of the Tata\n",
        "Institute of Fundamental Research (TIFR) and was instrumental in establishing the\n",
        "Atomic Energy Commission of India.”\n",
        "\n",
        "Write a Python program that processes this text using spaCy, then prints tokens, their\n",
        "lemmas, and any named entities found.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:"
      ],
      "metadata": {
        "id": "-eT4g8IZX6ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Given paragraph\n",
        "text = (\n",
        "    \"Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the \"\n",
        "    \"development of India’s atomic energy program. He was the founding director of the \"\n",
        "    \"Tata Institute of Fundamental Research (TIFR) and was instrumental in establishing \"\n",
        "    \"the Atomic Energy Commission of India.\"\n",
        ")\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print tokens and lemmas\n",
        "print(\"Tokens and Lemmas:\")\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<15} -> {token.lemma_}\")\n",
        "\n",
        "# Print named entities\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} ({ent.label_})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-TbuXgAYS61",
        "outputId": "3040a59e-2380-4625-dc78-0ff9f2b8fdbc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens and Lemmas:\n",
            "Homi            -> Homi\n",
            "Jehangir        -> Jehangir\n",
            "Bhaba           -> Bhaba\n",
            "was             -> be\n",
            "an              -> an\n",
            "Indian          -> indian\n",
            "nuclear         -> nuclear\n",
            "physicist       -> physicist\n",
            "who             -> who\n",
            "played          -> play\n",
            "a               -> a\n",
            "key             -> key\n",
            "role            -> role\n",
            "in              -> in\n",
            "the             -> the\n",
            "development     -> development\n",
            "of              -> of\n",
            "India           -> India\n",
            "’s              -> ’s\n",
            "atomic          -> atomic\n",
            "energy          -> energy\n",
            "program         -> program\n",
            ".               -> .\n",
            "He              -> he\n",
            "was             -> be\n",
            "the             -> the\n",
            "founding        -> found\n",
            "director        -> director\n",
            "of              -> of\n",
            "the             -> the\n",
            "Tata            -> Tata\n",
            "Institute       -> Institute\n",
            "of              -> of\n",
            "Fundamental     -> Fundamental\n",
            "Research        -> Research\n",
            "(               -> (\n",
            "TIFR            -> TIFR\n",
            ")               -> )\n",
            "and             -> and\n",
            "was             -> be\n",
            "instrumental    -> instrumental\n",
            "in              -> in\n",
            "establishing    -> establish\n",
            "the             -> the\n",
            "Atomic          -> Atomic\n",
            "Energy          -> Energy\n",
            "Commission      -> Commission\n",
            "of              -> of\n",
            "India           -> India\n",
            ".               -> .\n",
            "\n",
            "Named Entities:\n",
            "Homi Jehangir Bhaba (FAC)\n",
            "Indian (NORP)\n",
            "India (GPE)\n",
            "the Tata Institute of Fundamental Research (ORG)\n",
            "the Atomic Energy Commission of India (ORG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working on a chatbot for a mental health platform. Explain how\n",
        "you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford\n",
        "NLP to understand and respond to user input effectively. Detail your architecture, data\n",
        "preprocessing pipeline, and any ethical considerations.\n",
        "\n",
        "(Include your Python code and output in the code box below.)\n",
        "\n",
        "Answer:  "
      ],
      "metadata": {
        "id": "PmP-V-ARYuzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample mental-health related dataset\n",
        "texts = [\n",
        "    \"I feel very anxious today\",\n",
        "    \"I am happy and relaxed\",\n",
        "    \"I feel sad and overwhelmed\",\n",
        "    \"Everything is going well\"\n",
        "]\n",
        "\n",
        "# Labels: 1 = Distressed, 0 = Not distressed\n",
        "labels = [1, 0, 1, 0]\n",
        "\n",
        "# spaCy preprocessing (lemmatization)\n",
        "processed_texts = []\n",
        "for text in texts:\n",
        "    doc = nlp(text)\n",
        "    lemmas = \" \".join([token.lemma_ for token in doc])\n",
        "    processed_texts.append(lemmas)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(processed_texts)\n",
        "sequences = tokenizer.texts_to_sequences(processed_texts)\n",
        "\n",
        "# Padding\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "X = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "y = np.array(labels)\n",
        "\n",
        "# Build LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=16),\n",
        "    LSTM(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=10, verbose=1)\n",
        "\n",
        "# Test input\n",
        "test_text = \"I am feeling very stressed and tired\"\n",
        "doc = nlp(test_text)\n",
        "test_lemmas = \" \".join([token.lemma_ for token in doc])\n",
        "test_seq = tokenizer.texts_to_sequences([test_lemmas])\n",
        "test_pad = pad_sequences(test_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "prediction = model.predict(test_pad)\n",
        "print(\"Distress probability:\", prediction)\n"
      ],
      "metadata": {
        "id": "Sq05FUorZFPJ",
        "outputId": "b66a4a16-338c-4a85-c401-ce17f1722d0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5000 - loss: 0.6940\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 0.5000 - loss: 0.6930\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.5000 - loss: 0.6920\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - accuracy: 0.7500 - loss: 0.6909\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 1.0000 - loss: 0.6899\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - accuracy: 1.0000 - loss: 0.6888\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 1.0000 - loss: 0.6876\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - accuracy: 1.0000 - loss: 0.6864\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 1.0000 - loss: 0.6850\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - accuracy: 1.0000 - loss: 0.6836\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 271ms/step\n",
            "Distress probability: [[0.5038477]]\n"
          ]
        }
      ]
    }
  ]
}